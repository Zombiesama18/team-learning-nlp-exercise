{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超大图的学习\n",
    "在超大图上进行图神经网络的训练仍然具有挑战。普通的基于SGD的图神经网络的训练方法，要么面临着随着图神经网络层数增加，计算成本呈指数增长的问题，要么面临着保存整个图的信息和每一层每个节点的表征到内存（显存）而消耗巨大内存（显存）空间的问题。虽然已经有一些论文提出了无需保存整个图的信息和每一层每个节点的表征到GPU内存（显存）的方法，但这些方法可能会损失预测精度或者对提高内存的利用率并不明显。于是论文[Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Network](https://arxiv.org/abs/1905.07953)提出了一种新的图神经网络的训练方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Reddit\n",
    "from torch_geometric.loader import ClusterData, ClusterLoader, NeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "232965\n",
      "114615892\n",
      "602\n"
     ]
    }
   ],
   "source": [
    "dataset = Reddit('../datasets/Reddit')\n",
    "data = dataset[0]\n",
    "print(dataset.num_classes)\n",
    "print(data.num_nodes)\n",
    "print(data.num_edges)\n",
    "print(data.num_features)\n",
    "\n",
    "clustered_data = ClusterData(data, num_parts=1500, recursive=False, save_dir=dataset.processed_dir)\n",
    "train_loader = ClusterLoader(clustered_data, batch_size=16, shuffle=True)\n",
    "subgraph_loader = NeighborSampler(data.edge_index, sizes=[-1], batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2482\n",
      "139364\n",
      "tensor(1528)\n",
      "tensor([ 0,  1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 13, 15, 16, 18, 19, 21, 22,\n",
      "        23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40])\n"
     ]
    }
   ],
   "source": [
    "data = next(train_loader._get_iterator())\n",
    "print(data.num_nodes)\n",
    "print(data.num_edges)\n",
    "print(data.train_mask.sum())\n",
    "print(data.y.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.convs = torch.nn.ModuleList(\n",
    "            [SAGEConv(in_channels, 128),\n",
    "             SAGEConv(128, out_channels)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.nn.functional.relu(self.convs[0](x, edge_index))\n",
    "        x = torch.nn.functional.dropout(x, p=0.5, training=self.training)\n",
    "        return torch.nn.functional.log_softmax(self.convs[1](x, edge_index), dim=-1)\n",
    "    \n",
    "    def inference(self, x_all, subgraph_loader, device):\n",
    "        pbar = tqdm(total=x_all.size(0))\n",
    "        pbar.set_description('Evaluation')\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            xs = []\n",
    "            for batch_size, n_id, adj in subgraph_loader:\n",
    "                edge_index, _, size = adj.to(device)\n",
    "                x = x_all[n_id].to(device)\n",
    "                x_target = x[:size[1]]\n",
    "                x = conv((x, x_target), edge_index)\n",
    "                if i != len(self.convs) - 1:\n",
    "                    x = torch.nn.functional.relu(x)\n",
    "                xs.append(x.cpu())\n",
    "                pbar.update(batch_size // 2)\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "        pbar.close()\n",
    "        return x_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = total_nodes = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = torch.nn.functional.nll_loss(out[batch.train_mask], batch.y[batch.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        nodes = batch.train_mask.sum().item()\n",
    "        total_loss += loss.item() * nodes\n",
    "        total_nodes += nodes\n",
    "    return total_loss / total_nodes\n",
    "\n",
    "def test(model, data, device):\n",
    "    model.eval()\n",
    "    data = data.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.inference(data.x, subgraph_loader, device)\n",
    "        y_pred = out.argmax(dim=-1)\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = y_pred[mask].eq(data.y[mask]).sum().item()\n",
    "        accs.append(correct / mask.sum().item())\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.0917\n",
      "Epoch: 002, Loss: 0.4706\n",
      "Epoch: 003, Loss: 0.4019\n",
      "Epoch: 004, Loss: 0.3824\n",
      "Epoch: 005, Loss: 0.3514\n",
      "Epoch: 006, Loss: 0.3272\n",
      "Epoch: 007, Loss: 0.3233\n",
      "Epoch: 008, Loss: 0.3291\n",
      "Epoch: 009, Loss: 0.3125\n",
      "Epoch: 010, Loss: 0.2991\n",
      "Epoch: 011, Loss: 0.2925\n",
      "Epoch: 012, Loss: 0.2908\n",
      "Epoch: 013, Loss: 0.2996\n",
      "Epoch: 014, Loss: 0.3067\n",
      "Epoch: 015, Loss: 0.2975\n",
      "Epoch: 016, Loss: 0.2847\n",
      "Epoch: 017, Loss: 0.2721\n",
      "Epoch: 018, Loss: 0.2719\n",
      "Epoch: 019, Loss: 0.2832\n",
      "Epoch: 020, Loss: 0.2860\n",
      "Epoch: 021, Loss: 0.2684\n",
      "Epoch: 022, Loss: 0.2728\n",
      "Epoch: 023, Loss: 0.2783\n",
      "Epoch: 024, Loss: 0.2618\n",
      "Epoch: 025, Loss: 0.2610\n",
      "Epoch: 026, Loss: 0.2980\n",
      "Epoch: 027, Loss: 0.2545\n",
      "Epoch: 028, Loss: 0.2459\n",
      "Epoch: 029, Loss: 0.2423\n",
      "Epoch: 030, Loss: 0.2435\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "model = Net(dataset.num_features, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(30):\n",
    "    loss = train(model, train_loader, optimizer)\n",
    "    # if epoch % 5 == 0:\n",
    "    #     train_acc, val_acc, test_acc = test(model, data, device)\n",
    "    #     print(f'Epoch: {epoch + 1:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "    # else:\n",
    "    print(f'Epoch: {epoch + 1:03d}, Loss: {loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
