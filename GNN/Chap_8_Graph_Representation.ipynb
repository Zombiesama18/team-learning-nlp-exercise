{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, GlobalAttention, Set2Set\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "from torch_geometric.nn import MessagePassing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于图同构网络（GIN）的图表征网络的实现\n",
    "基于图同构网络（Graph Isomorphism Network, GIN）的图表征网络是当前最经典的图表征学习网络\n",
    "基于图同构网络的图表征学习主要包含以下两个过程：\n",
    "1. 首先计算得到节点表征；\n",
    "2. 其次对图上各个节点的表征做图池化（Graph Pooling），或称为图读出（Graph Readout），得到图的表征（Graph Representation）。\n",
    "\n",
    "`ogb.graphpropred.mol_encoder` 中 `AtomEncoder` 和 `BondEncoder` 的使用：\n",
    "\n",
    "### 基于图同构网络的图表征模块（GINGraphRepr Module）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    torch_geometric.nn.GINConv 不支持存在边属性的图\n",
    "    Args:\n",
    "        MessagePassing (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim):\n",
    "        super(GINConv, self).__init__(aggr = \"add\")\n",
    "\n",
    "        self.mlp = torch.nn.Sequential(torch.nn.Linear(emb_dim, emb_dim), torch.nn.BatchNorm1d(emb_dim), torch.nn.ReLU(), torch.nn.Linear(emb_dim, emb_dim))\n",
    "        self.eps = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.bond_encoder = BondEncoder(emb_dim = emb_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        edge_embedding = self.bond_encoder(edge_attr) # 先将类别型边属性转换为边嵌入\n",
    "        out = self.mlp((1 + self.eps) *x + self.propagate(edge_index, x=x, edge_attr=edge_embedding))\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        return torch.nn.functional.relu(x_j + edge_attr)\n",
    "        \n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "class GINNodeEmbedding(torch.nn.Module):\n",
    "    def __init__(self, num_layers, emb_dim, drop_ratio=0.5, JK=\"last\", residual=False, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "        self.residual = residual\n",
    "        \n",
    "        if self.num_layers < 2:\n",
    "            raise ValueError(\"Number of GNN layers mush be greater than 1\")\n",
    "        \n",
    "        self.atom_encoder = AtomEncoder(emb_dim)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        \n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(GINConv(emb_dim))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))\n",
    "    \n",
    "    def forward(self, batched_data):\n",
    "        x, edge_index, edge_attr = batched_data.x, batched_data.edge_index, batched_data.edge_attr\n",
    "        \n",
    "        h_list = [self.atom_encoder(x)]\n",
    "        for layer in range(self.num_layers):\n",
    "            h = self.convs[layer](h_list[layer], edge_index, edge_attr)\n",
    "            h = self.batch_norms[layer](h)\n",
    "            if layer == self.num_layers - 1:\n",
    "                h = torch.nn.functional.dropout(h, self.drop_ratio, training=self.training)\n",
    "            else:\n",
    "                h = torch.nn.functional.dropout(torch.nn.functional.relu(h), self.drop_ratio, training=self.training)\n",
    "            if self.residual:\n",
    "                h += h_list[layer]\n",
    "            h_list.append(h)\n",
    "        \n",
    "        if self.JK == 'last':\n",
    "            node_representation = h_list[-1]\n",
    "        elif self.JK == 'sum':\n",
    "            node_representation = 0\n",
    "            for layer in range(self.num_layers - 1):\n",
    "                node_representation += h_list[layer]\n",
    "        return node_representation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
